{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Reading Data and Importing Libraries ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in question df\nquestions_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv',                         \n                            usecols=[0, 3],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8'}\n                          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in lecture df\nlectures_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n\npart_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lectures_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge lecture features to train dataset\ntrain_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lectures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collect per user stats\nuser_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_lecture_stats_part.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add boolean features\nfor column in user_lecture_stats_part.columns:\n    bool_column = column + '_boolean'\n    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_lecture_stats_part.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clearing memory\ndel(train_lectures)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing True or 1 for content_type_id\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.task_container_id == 9999)].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.content_type_id == False)].task_container_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 / group2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group3.iloc[0].avg_questions_seen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u2_final.explanation_mean_user.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question2.quest_pct = round(question2.quest_pct,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(question2.head(), question2.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_mean_user = results_u2_final.explanation_mean_user.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Validation Set (Most Recent Answers by User) ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\nI am convinced there is a better way to match the test data."},{"metadata":{},"cell_type":"markdown","source":"## Extracting Training Data ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.answered_correctly.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging Data ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"#clearing memory\ndel(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n\nX = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n\nvalidation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in question df\n#question2 = pd.read_csv('/kaggle/input/question2/question2.csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n       'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]\nX_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n               'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n               'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Filling with 0.5 for simplicity; there could likely be a better value\nX['answered_correctly_user'].fillna(0.65,  inplace=True)\nX['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX['quest_pct'].fillna(content_mean, inplace=True)\n\nX['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX['part_1'].fillna(0, inplace = True)\nX['part_2'].fillna(0, inplace = True)\nX['part_3'].fillna(0, inplace = True)\nX['part_4'].fillna(0, inplace = True)\nX['part_5'].fillna(0, inplace = True)\nX['part_6'].fillna(0, inplace = True)\nX['part_7'].fillna(0, inplace = True)\nX['type_of_concept'].fillna(0, inplace = True)\nX['type_of_intention'].fillna(0, inplace = True)\nX['type_of_solving_question'].fillna(0, inplace = True)\nX['type_of_starter'].fillna(0, inplace = True)\nX['part_1_boolean'].fillna(0, inplace = True)\nX['part_2_boolean'].fillna(0, inplace = True)\nX['part_3_boolean'].fillna(0, inplace = True)\nX['part_4_boolean'].fillna(0, inplace = True)\nX['part_5_boolean'].fillna(0, inplace = True)\nX['part_6_boolean'].fillna(0, inplace = True)\nX['part_7_boolean'].fillna(0, inplace = True)\nX['type_of_concept_boolean'].fillna(0, inplace = True)\nX['type_of_intention_boolean'].fillna(0, inplace = True)\nX['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX['type_of_starter_boolean'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX_val['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX_val['part_1'].fillna(0, inplace = True)\nX_val['part_2'].fillna(0, inplace = True)\nX_val['part_3'].fillna(0, inplace = True)\nX_val['part_4'].fillna(0, inplace = True)\nX_val['part_5'].fillna(0, inplace = True)\nX_val['part_6'].fillna(0, inplace = True)\nX_val['part_7'].fillna(0, inplace = True)\nX_val['type_of_concept'].fillna(0, inplace = True)\nX_val['type_of_intention'].fillna(0, inplace = True)\nX_val['type_of_solving_question'].fillna(0, inplace = True)\nX_val['type_of_starter'].fillna(0, inplace = True)\nX_val['part_1_boolean'].fillna(0, inplace = True)\nX_val['part_2_boolean'].fillna(0, inplace = True)\nX_val['part_3_boolean'].fillna(0, inplace = True)\nX_val['part_4_boolean'].fillna(0, inplace = True)\nX_val['part_5_boolean'].fillna(0, inplace = True)\nX_val['part_6_boolean'].fillna(0, inplace = True)\nX_val['part_7_boolean'].fillna(0, inplace = True)\nX_val['type_of_concept_boolean'].fillna(0, inplace = True)\nX_val['type_of_intention_boolean'].fillna(0, inplace = True)\nX_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_val['type_of_starter_boolean'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(X)\nX_val = scaler.transform(X_val)\nX_train = X.reshape(X.shape[0], X.shape[1], 1)\nX_test = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.metrics import BinaryAccuracy\nfrom keras import backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 32 \nX_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 32  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nff_dim = 64  # Hidden layer size in feed forward network inside transformer\nvocab_size = 50000\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_auc', mode='max',patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\"adam\", \"binary_crossentropy\", metrics=[tf.keras.metrics.AUC()])\n\nhistory = model.fit(\n    X_train, y, epochs=25,verbose=1,validation_split=0.1,batch_size=65536,callbacks=[es]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_true = np.array(y_val)\nroc_auc_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions for New Data ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\nenv = riiideducation.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    \n    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n    test_df['part_1'].fillna(0, inplace = True)\n    test_df['part_2'].fillna(0, inplace = True)\n    test_df['part_3'].fillna(0, inplace = True)\n    test_df['part_4'].fillna(0, inplace = True)\n    test_df['part_5'].fillna(0, inplace = True)\n    test_df['part_6'].fillna(0, inplace = True)\n    test_df['part_7'].fillna(0, inplace = True)\n    test_df['type_of_concept'].fillna(0, inplace = True)\n    test_df['type_of_intention'].fillna(0, inplace = True)\n    test_df['type_of_solving_question'].fillna(0, inplace = True)\n    test_df['type_of_starter'].fillna(0, inplace = True)\n    test_df['part_1_boolean'].fillna(0, inplace = True)\n    test_df['part_2_boolean'].fillna(0, inplace = True)\n    test_df['part_3_boolean'].fillna(0, inplace = True)\n    test_df['part_4_boolean'].fillna(0, inplace = True)\n    test_df['part_5_boolean'].fillna(0, inplace = True)\n    test_df['part_6_boolean'].fillna(0, inplace = True)\n    test_df['part_7_boolean'].fillna(0, inplace = True)\n    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n    \n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    X = test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n                                                            'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n                                                            'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n                                                            'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n                                                            'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]\n    X=scaler.transform(X)\n    X = X.reshape(X.shape[0], X.shape[1], 1)\n    X = keras.preprocessing.sequence.pad_sequences(X, maxlen=maxlen)\n    test_df['answered_correctly'] =  model.predict(X)\n\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}