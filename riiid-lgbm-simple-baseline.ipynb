{
 "cells": [
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#import riiideducation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "#env = riiideducation.make_env()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('../input/riiid-test-answer-prediction/train.csv', low_memory=False, nrows=10**7,\n",
    "                      dtype={\n",
    "                          'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n",
    "                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n",
    "                             'prior_question_had_explanation': 'boolean',\n",
    "                      })\n",
    "\n",
    "# train_df = train_df.query('answered_correctly != -1').reset_index(drop=True)\n",
    "# train_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype(float) 後でやる"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "features_part_df と train_part_df に分ける。"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 900万行・・・特徴量作成\n",
    "features_part_df = train_df.iloc[:int( 9 / 10 * len(train_df) )]\n",
    "# 100万行・・・最新の100万件 \n",
    "train_part_df = train_df.iloc[int( 9 / 10 * len(train_df) ):]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_part_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "features_part_df で新しい特徴量を作成"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 追加するデータ1 user_answers_df\n",
    "train_questions_only_df = features_part_df[features_part_df['answered_correctly'] != -1]\n",
    "\n",
    "grouped_by_user_df = train_questions_only_df.groupby('user_id')\n",
    "\n",
    "user_answers_df = grouped_by_user_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew']}).copy()\n",
    "user_answers_df.columns = ['mean_user_accuracy', 'questions_answered', 'std_user_accuracy', 'median_user_accuracy', 'skew_user_accuracy']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "user_answers_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 追加するデータ2 questions_df\n",
    "questions_df = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n",
    "\n",
    "grouped_by_content_df = train_questions_only_df.groupby('content_id')\n",
    "\n",
    "content_answers_df = grouped_by_content_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew'] }).copy()\n",
    "content_answers_df.columns = ['mean_accuracy', 'question_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy']\n",
    "\n",
    "questions_df = questions_df.merge(content_answers_df, left_on = 'question_id', right_on = 'content_id', how = 'left')\n",
    "\n",
    "bundle_dict = questions_df['bundle_id'].value_counts().to_dict()\n",
    "\n",
    "# right_answers 正解数\n",
    "questions_df['right_answers'] = questions_df['mean_accuracy'] * questions_df['question_asked']\n",
    "\n",
    "questions_df['bundle_size'] = questions_df['bundle_id'].apply(lambda x: bundle_dict[x])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "questions_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 追加するデータ3 bundle_answers_df\n",
    "grouped_by_bundle_df = questions_df.groupby('bundle_id')\n",
    "\n",
    "bundle_answers_df = grouped_by_bundle_df.agg({'right_answers': 'sum', 'question_asked': 'sum'}).copy()\n",
    "bundle_answers_df.columns = ['bundle_right_answers', 'bundle_questions_asked']\n",
    "\n",
    "bundle_answers_df['bundle_accuracy'] = bundle_answers_df['bundle_right_answers'] / bundle_answers_df['bundle_questions_asked']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "bundle_answers_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 追加するデータ4 part_answers_df\n",
    "grouped_by_part_df = questions_df.groupby('part')\n",
    "\n",
    "part_answers_df = grouped_by_part_df.agg({'right_answers': 'sum', 'question_asked': 'sum'}).copy()\n",
    "\n",
    "part_answers_df.columns = ['part_right_answers', 'part_questions_asked']\n",
    "part_answers_df['part_accuracy'] = part_answers_df['part_right_answers'] / part_answers_df['part_questions_asked']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "part_answers_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 旧\n",
    "# features = [\n",
    "#     'timestamp','mean_user_accuracy', 'questions_answered','mean_accuracy',\n",
    "#     'question_asked','prior_question_elapsed_time', 'prior_question_had_explanation',\n",
    "#     'bundle_size', 'bundle_accuracy','part_accuracy', 'right_answers'\n",
    "# ]\n",
    "\n",
    "# 新\n",
    "features = [\n",
    "    'timestamp','prior_question_elapsed_time', 'prior_question_had_explanation',\n",
    "    'mean_user_accuracy', 'questions_answered', 'std_user_accuracy',\n",
    "    'median_user_accuracy', 'skew_user_accuracy','mean_accuracy',\n",
    "    'question_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy',\n",
    "    'bundle_size','bundle_accuracy', 'part_accuracy'\n",
    "]\n",
    "\n",
    "target = 'answered_correctly'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# 講義(-1)以外を抽出 train\n",
    "train_part_df = train_part_df[train_part_df[target] != -1]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# user_answers_df\n",
    "train_part_df = train_part_df.merge(user_answers_df, how='left', on='user_id')\n",
    "\n",
    "# questions_df\n",
    "train_part_df = train_part_df.merge(questions_df, how='left', left_on='content_id', right_on='question_id')\n",
    "\n",
    "# bundle_answers_df\n",
    "train_part_df = train_part_df.merge(bundle_answers_df, how='left', on='bundle_id')\n",
    "\n",
    "# part_answers_df\n",
    "train_part_df = train_part_df.merge(part_answers_df, how='left', on='part')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# ユーザーが質問に回答した後、説明と正しい回答を確認したかどうか 欠損値をFalseと置く、 astypeでデータ型の変換(キャスト)\n",
    "train_part_df['prior_question_had_explanation'] = train_part_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n",
    "\n",
    "train_part_df.fillna(value = -1, inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_part_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_part_df.columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "X_train = train_part_df[features]\n",
    "y_train = train_part_df[target]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "X_train"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "models = []\n",
    "oof_train = np.zeros(len(X_train),) ### array([0., 0., 0., ..., 0., 0., 0.])\n",
    "categorical_features = ['prior_question_had_explanation']\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'max_bin': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 40\n",
    "}\n",
    "\n",
    "n_tr = round(981094 * 0.9)\n",
    "\n",
    "X_tr = X_train[:n_tr]\n",
    "X_val = X_train[n_tr:]\n",
    "\n",
    "y_tr = y_train[:n_tr]\n",
    "y_val = y_train[n_tr:]\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_eval],\n",
    "    verbose_eval=10,\n",
    "    num_boost_round=1000,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "oof_train = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "models.append(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# ROC曲線のAUCスコア・・・曲線下の面積を意味するらしい。\n",
    "roc_auc_score(y_val, oof_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "@iter_test = env.iter_test()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "for (test_df, sample_prediction_df) in iter_test:\n    y_preds = []\n    \n    test_df = test_df.merge(user_answers_df, how = 'left', on = 'user_id')\n    test_df = test_df.merge(questions_df, how = 'left', left_on = 'content_id', right_on = 'question_id')\n    test_df = test_df.merge(bundle_answers_df, how = 'left', on = 'bundle_id')\n    test_df = test_df.merge(part_answers_df, how = 'left', on = 'part')\n    \n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_df.fillna(value = -1, inplace = True)\n    X_test = test_df[features]\n    \n    for model in models:\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n        y_preds.append(y_pred)\n        \n    y_preds = sum(y_preds) / len(y_preds)\n    test_df['answered_correctly'] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## discussion"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Knowledge tracing (KT)について\n\nKnowledge tracing (KT) refers to the problem of predicting future learner performance given their past performance in educational applications\n\nEach student's knowledge is modeled by estimating the performance of the student on the learning activities.\n\nKnowledgeTracing、EdNet, Artificial Intelligence in Education (AIEd)に関する記事\nhttps://www.kaggle.com/c/riiid-test-answer-prediction/discussion/188911\n\n* 時制に関して\n\n時制データには、ターゲットエンコーディングをすることが重要。(https://maxhalford.github.io/blog/pandas-tricks/#target-encoding-for-time-series)\n\nターゲットエンコーディングについて (https://maxhalford.github.io/blog/target-encoding/)\n\nThe one thing I do in every time series competition is target encoding. In short, the goal of target encoding is to replace a category by the average of the target values of the rows that belong to said category. Naturally, you’re not limited to using an average. You can also use Bayesian target encoding if some of your categories are rare.\n\nTarget encoding is very important for time series data. It has been used in almost every top model on Kaggle time series competitions, such as ASHRAE, Recruit Restaurants, and Wikipedia Web Traffic. When you do target encoding on temporal data, you need to be wary of not leaking current and future information into the aggregate of each row. Basically, you need to shift the target values within each group backwards. Indeed, for any given moment, we want our aggregate to pertain to past data only.\n\n参考ディスカション (https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/189437)\n\n* Timestamp と prior_question_elaped_time について\n\nhttps://www.kaggle.com/c/riiid-test-answer-prediction/discussion/189351\n　"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}